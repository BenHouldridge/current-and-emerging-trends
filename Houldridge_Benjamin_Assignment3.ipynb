{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizing CIFAR-10 Images\n",
    "### imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10 \n",
    "from keras.utils import np_utils \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D \n",
    "from keras.optimizers import SGD, Adam, RMSprop \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32 \n",
    "IMG_COLS = 32\n",
    "BATCH_SIZE = 128 \n",
    "NB_EPOCH = 20 \n",
    "NB_CLASSES = 10 \n",
    "VERBOSE = 1 \n",
    "VALIDATION_SPLIT = 0.2 \n",
    "OPTIM = RMSprop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 3s 0us/step\n",
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() \n",
    "print('X_train shape:', X_train.shape) \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to categorical \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "\n",
    "# float and normalization \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "X_train /= 255 \n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 4,200,842\n",
      "Trainable params: 4,200,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() \n",
    "model.add(Conv2D(32, (3, 3), padding='same',input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS))) \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(512)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 56s 1ms/step - loss: 1.7026 - accuracy: 0.3988 - val_loss: 1.3591 - val_accuracy: 0.5268\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 59s 1ms/step - loss: 1.3362 - accuracy: 0.5289 - val_loss: 1.2889 - val_accuracy: 0.5408\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 1.2106 - accuracy: 0.5726 - val_loss: 1.3583 - val_accuracy: 0.5354\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 1.1192 - accuracy: 0.6094 - val_loss: 1.1305 - val_accuracy: 0.6053\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 50s 1ms/step - loss: 1.0420 - accuracy: 0.6317 - val_loss: 1.0692 - val_accuracy: 0.6257\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 50s 1ms/step - loss: 0.9874 - accuracy: 0.6553 - val_loss: 1.0645 - val_accuracy: 0.6329\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 60s 2ms/step - loss: 0.9321 - accuracy: 0.6763 - val_loss: 1.0757 - val_accuracy: 0.6299\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 0.8830 - accuracy: 0.6919 - val_loss: 1.0685 - val_accuracy: 0.6458\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 0.8453 - accuracy: 0.7061 - val_loss: 1.1550 - val_accuracy: 0.6143\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 0.8052 - accuracy: 0.7195 - val_loss: 1.0139 - val_accuracy: 0.6624\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 0.7686 - accuracy: 0.7346 - val_loss: 1.0377 - val_accuracy: 0.6566\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 0.7337 - accuracy: 0.7465 - val_loss: 1.1276 - val_accuracy: 0.6449\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.7097 - accuracy: 0.7560 - val_loss: 0.9892 - val_accuracy: 0.6777\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 53s 1ms/step - loss: 0.6813 - accuracy: 0.7648 - val_loss: 1.1042 - val_accuracy: 0.6487\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 0.6504 - accuracy: 0.7760 - val_loss: 1.0057 - val_accuracy: 0.6710\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 0.6264 - accuracy: 0.7851 - val_loss: 1.1438 - val_accuracy: 0.6442\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 52s 1ms/step - loss: 0.6027 - accuracy: 0.7922 - val_loss: 1.0895 - val_accuracy: 0.6722\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 58s 1ms/step - loss: 0.5826 - accuracy: 0.8011 - val_loss: 1.0431 - val_accuracy: 0.6702\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 57s 1ms/step - loss: 0.5650 - accuracy: 0.8083 - val_loss: 1.0897 - val_accuracy: 0.6779\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 51s 1ms/step - loss: 0.5416 - accuracy: 0.8164 - val_loss: 1.0457 - val_accuracy: 0.6871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x213cc9a4388>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=OPTIM, \n",
    "              metrics=['accuracy']) \n",
    "model.fit(X_train, \n",
    "          Y_train, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          epochs=NB_EPOCH, \n",
    "          validation_split=VALIDATION_SPLIT, \n",
    "          verbose=VERBOSE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 472us/step\n",
      "Test score: 1.0323203575134277\n",
      "Test accuracy: 0.6801999807357788\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, \n",
    "                       Y_test, \n",
    "                       batch_size=BATCH_SIZE, \n",
    "                       verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json() \n",
    "\n",
    "with open('cifar10_architecture.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "  \n",
    "model.save_weights('cifar10_weights.h5', overwrite=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics\n",
    "\n",
    "    The ethical concerns that arise from AI image recognition can be broadly grouped into two categories. One encompassed the gathering and labeling of training data, and the other is the use and proliferation of the trained model. Datasets of training images need to be large. The easiest way to get a large data set is by scraping the internet. Just because images are free to view online does not mean it’s ethical to use them in a training dataset. People often don’t want their faces used in training and artists don’t want their work used either. Without consent and licences is it ethical to use and profit off of other people’s images? Scraping data also leads to biased datasets, and potentially incorrect labeling. The internet is full of falsehoods and training models on this data unfiltered will lead to models that perpetuate these falsehoods. This has occurred in self-driving. Many cars have identification bias that increases the risk of minorities being misidentified and therefore harmed at a higher rate (Kim 2021). \n",
    "    Even if a dataset is ethically sourced, once a model is trained it can be used in unethical ways. Identification of persons infringes on people's privacy and is feared will be used to discriminate and target groups. This could be an oppressive government using AI to identify and round up a minority population, or a company using identification to target gambling ads to people who display gambling addiction at a casino (Nature 2020). \n",
    "    The ability for machines to identify images has many potential benefits but also ethical issues that can cause damage to our society. We must be careful in our approach to these technologies.\n",
    "\n",
    "\n",
    "\n",
    "# References\n",
    "\n",
    "Krizhevsky, A. (2009). Learning Multiple Layers of Features from Tiny Images. https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf\n",
    "\n",
    "Gulli, A., & Pal, S. (2017). Deep learning with Keras : implement neural networks with Keras on Theano and TensorFlow. Packt Publishing.\n",
    "\n",
    "Kim, T. (2021, October 11). Opinion: Biased Algorithms Are Training Self-Driving Cars. GovTech. https://www.govtech.com/opinion/opinions-biased-algorithms-are-training-self-driving-cars\n",
    "\n",
    "Nature. (2020). Facial-recognition research needs an ethical reckoning. Nature, 587(7834), 330–330. https://doi.org/10.1038/d41586-020-03256-7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
